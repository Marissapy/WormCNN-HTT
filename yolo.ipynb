{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "  File \"/tmp/ipykernel_333733/677302030.py\", line 1, in <module>\n",
      "    from ultralytics import YOLO\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/__init__.py\", line 11, in <module>\n",
      "    from ultralytics.models import NAS, RTDETR, SAM, YOLO, FastSAM, YOLOWorld\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/models/__init__.py\", line 3, in <module>\n",
      "    from .fastsam import FastSAM\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/models/fastsam/__init__.py\", line 3, in <module>\n",
      "    from .model import FastSAM\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/models/fastsam/model.py\", line 5, in <module>\n",
      "    from ultralytics.engine.model import Model\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 11, in <module>\n",
      "    from ultralytics.cfg import TASK2DATA, get_cfg, get_save_dir\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 12, in <module>\n",
      "    from ultralytics.utils import (\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/utils/__init__.py\", line 23, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/matplotlib/__init__.py\", line 1270, in <module>\n",
      "    rcParams['backend'] = os.environ.get('MPLBACKEND')\n",
      "    ~~~~~~~~^^^^^^^^^^^\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/matplotlib/__init__.py\", line 738, in __setitem__\n",
      "    raise ValueError(f\"Key {key}: {ve}\") from None\n",
      "ValueError: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1136, in get_records\n",
      "  File \"/home/spuser/.conda/envs/yanpan/lib/python3.11/site-packages/pygments/styles/__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# 加载预训练的 YOLOv8 模型（替换为所需的模型大小）\n",
    "model = YOLO('/data/yan/track/yolomodel/yolo11x.pt')\n",
    "# 开始训练\n",
    "model.train(\n",
    "    data='./data.yaml',  # 数据集配置文件路径\n",
    "    epochs=512,         # 训练轮数\n",
    "    imgsz=640,          # 输入图片尺寸\n",
    "    batch=24,           # 批量大小，根据GPU内存调整\n",
    "    lr0=0.01,           # 初始学习率\n",
    "    name='yolov11_WormCNN-HTT',  # 训练名称，用于保存模型\n",
    "    project='runs/detect',     # 保存路径\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # 加载训练好的模型\n",
    "# model = YOLO('runs/detect/yolov11_WormCNN-HTT7/weights/best.pt')  # 替换为您的最佳模型路径\n",
    "\n",
    "# # 打开视频或摄像头\n",
    "# video_path = './vedio/TS-20241123220321351.avi'  # 或使用 0 代表摄像头\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # 进行检测\n",
    "#     results = model(frame)\n",
    "\n",
    "#     # 解析并绘制检测结果\n",
    "#     annotated_frame = results[0].plot()\n",
    "\n",
    "#     # 显示结果\n",
    "#     cv2.imshow('YOLOv8 Detection', annotated_frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频帧率: 29.3, 分辨率: 1600x1200, 总帧数: 293\n",
      "处理帧 1/293\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m处理完成，输出视频保存在: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m处理帧 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 进行检测\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 绘制检测结果\u001b[39;00m\n\u001b[1;32m     50\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/predictor.py:266\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/models/yolo/segment/predict.py:52\u001b[0m, in \u001b[0;36mSegmentationPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     masks \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mprocess_mask_native(proto[i], pred[:, \u001b[38;5;241m6\u001b[39m:], pred[:, :\u001b[38;5;241m4\u001b[39m], orig_img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# HWC\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# HWC\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     pred[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mscale_boxes(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], pred[:, :\u001b[38;5;241m4\u001b[39m], orig_img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(Results(orig_img, path\u001b[38;5;241m=\u001b[39mimg_path, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames, boxes\u001b[38;5;241m=\u001b[39mpred[:, :\u001b[38;5;241m6\u001b[39m], masks\u001b[38;5;241m=\u001b[39mmasks))\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/utils/ops.py:671\u001b[0m, in \u001b[0;36mprocess_mask\u001b[0;34m(protos, masks_in, bboxes, shape, upsample)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_mask\u001b[39m(protos, masks_in, bboxes, shape, upsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    657\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    Apply masks to bounding boxes using the output of the mask head.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m            are the height and width of the input image. The mask is applied to the bounding boxes.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     c, mh, mw \u001b[38;5;241m=\u001b[39m protos\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# CHW\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     ih, iw \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    673\u001b[0m     masks \u001b[38;5;241m=\u001b[39m (masks_in \u001b[38;5;241m@\u001b[39m protos\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(c, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mh, mw)  \u001b[38;5;66;03m# CHW\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # 加载训练好的模型\n",
    "    model_path = 'runs/segment/yolov11_WormCNN-HTT22/weights/best.pt'  # 替换为您的最佳模型路径\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {model_path}\")\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # 输入视频路径\n",
    "    video_path = './vedio/TS-20241123220321351.avi'  # 替换为您的视频路径\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"视频文件不存在: {video_path}\")\n",
    "    \n",
    "    # 打开视频\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"无法打开视频文件: {video_path}\")\n",
    "\n",
    "    # 获取视频参数\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"视频帧率: {fps}, 分辨率: {width}x{height}, 总帧数: {total_frames}\")\n",
    "\n",
    "    # 定义输出视频的保存路径和编码器\n",
    "    output_dir = 'output_videos'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, 'annotated_TS-20241123220321351.avi')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # 使用 XVID 编码器\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        print(f\"处理帧 {frame_count}/{total_frames}\", end='\\r')\n",
    "\n",
    "        # 进行检测\n",
    "        results = model(frame, imgsz=640)[0]\n",
    "\n",
    "        # 绘制检测结果\n",
    "        annotated_frame = results.plot()\n",
    "\n",
    "        # 写入输出视频\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "    # 释放资源\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\n处理完成，输出视频保存在: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/segment/yolov11_WormCNN-HTT22/weights/best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 进行预测\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/yan/track/datasets/val/images/TS-20241123222043114.avi_0002.tiff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 替换为你的输入数据\u001b[39;00m\n\u001b[1;32m      8\u001b[0m results\u001b[38;5;241m.\u001b[39mshow()  \u001b[38;5;66;03m# 显示结果\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/model.py:554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/engine/predictor.py:266\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/models/yolo/segment/predict.py:52\u001b[0m, in \u001b[0;36mSegmentationPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     masks \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mprocess_mask_native(proto[i], pred[:, \u001b[38;5;241m6\u001b[39m:], pred[:, :\u001b[38;5;241m4\u001b[39m], orig_img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# HWC\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# HWC\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     pred[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mscale_boxes(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], pred[:, :\u001b[38;5;241m4\u001b[39m], orig_img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(Results(orig_img, path\u001b[38;5;241m=\u001b[39mimg_path, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames, boxes\u001b[38;5;241m=\u001b[39mpred[:, :\u001b[38;5;241m6\u001b[39m], masks\u001b[38;5;241m=\u001b[39mmasks))\n",
      "File \u001b[0;32m~/.conda/envs/yanpan/lib/python3.11/site-packages/ultralytics/utils/ops.py:671\u001b[0m, in \u001b[0;36mprocess_mask\u001b[0;34m(protos, masks_in, bboxes, shape, upsample)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_mask\u001b[39m(protos, masks_in, bboxes, shape, upsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    657\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m    Apply masks to bounding boxes using the output of the mask head.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m            are the height and width of the input image. The mask is applied to the bounding boxes.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     c, mh, mw \u001b[38;5;241m=\u001b[39m protos\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# CHW\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     ih, iw \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    673\u001b[0m     masks \u001b[38;5;241m=\u001b[39m (masks_in \u001b[38;5;241m@\u001b[39m protos\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(c, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mh, mw)  \u001b[38;5;66;03m# CHW\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 加载 segment 模型\n",
    "model = YOLO('runs/segment/yolov11_WormCNN-HTT22/weights/best.pt')\n",
    "\n",
    "# 进行预测\n",
    "results = model('/data/yan/track/datasets/val/images/TS-20241123222043114.avi_0002.tiff')  # 替换为你的输入数据\n",
    "results.show()  # 显示结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yanpan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
